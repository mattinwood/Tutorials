{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[ec2-user@ip-###-##-##-## ~]$ mkdir airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### These may not be necessary depending on your system set up\n",
    "##### Exact syntax will vary by system"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[ec2-user@ip-###-##-##-## ~]$ sudo yum install python36-devel\n",
    "[ec2-user@ip-###-##-##-## ~]$ sudo yum install gcc"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[ec2-user@ip-###-##-##-## airflow]$ sudo pip-3.6 install airflow"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[ec2-user@ip-###-##-##-## ~]$ cd airflow\n",
    "[ec2-user@ip-###-##-##-## airflow]$ airflow\n",
    "[ec2-user@ip-###-##-##-## airflow]$ ls\n",
    "airflow.cfg  unittests.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No automatic restarting; add to boot, add cronjob to monitor and run"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[ec2-user@ip-###-##-##-## airflow]$ airflow initdb\n",
    "\n",
    "[ec2-user@ip-###-##-##-## airflow]$ mkdir etl\n",
    "[ec2-user@ip-###-##-##-## airflow]$ mkdir dags"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[ec2-user@ip-###-##-##-## airflow]$ airflow webserver -D\n",
    "[ec2-user@ip-###-##-##-## airflow]$ airflow scheduler -D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If publicly exposed, set up web authentication"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "authenticate = True\n",
    "auth_backend = airflow.contrib.auth.backends.password_auth"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ">>> import airflow\n",
    ">>> from airflow.contrib.auth.backends.password_auth import PasswordUser\n",
    ">>> user = PasswordUser(models.User())\n",
    ">>> user.username = 'matt'\n",
    ">>> user.email = 'matt.inwood@gmail.com'\n",
    ">>> user._set_password = 'password'\n",
    ">>> session = settings.Session()\n",
    ">>> session.add(user)\n",
    ">>> session.commit()\n",
    ">>> session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAGs\n",
    "#### Directed Acyclic Graph"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from airflow import DAG\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'ETL-User',\n",
    "    'depends_on_past': False,            # Requieres the previous instance of this DAG to be Completed Succesfully\n",
    "    'start_date': datetime(2018, 4, 5),  # If set in past and config set to backfill, will run from that date onward\n",
    "    'retries': 3                         # Number of times each individual \n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    'Dag_or_Job_Name',\n",
    "    default_args=default_args,           # Dictionary of arguments that apply to all Tasks assigned to this Dag\n",
    "    schedule_interval='*/2 * * * *',     # @command, or CRON expression\n",
    "    dagrun_timeout=timedelta(minutes=5), # Job is considered failed if it exists longer than specified\n",
    "    sla_miss_callback=sla_miss_msg)      # All callable functions do not include the parantheses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Task_Name = PythonOperator(\n",
    "    task_id='Task_Name',                 # Typically the same as the function name\n",
    "    provide_context=True,                # If true, passes a dictionary of keywords\n",
    "    op_kwargs={'key': 'value'},          # If passing context, adds additional keyword arguments\n",
    "    python_callable=python_func,         # Name of a python functions; without () or parameters\n",
    "    on_failure_callback=on_failure,\n",
    "    on_success_callback=on_success,\n",
    "    dag=dag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of all default keyword arguments can be found here:\n",
    "\n",
    "https://airflow.apache.org/code.html?highlight=kwargs#default-variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Operators"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Do not pass ** as part of kwargs\n",
    "def on_success(kwargs):\n",
    "    slack_message(\"Success: {0}: {1}\".format(kwargs['ts'], kwargs['task']))\n",
    "\n",
    "\n",
    "def on_failure(kwargs):\n",
    "    slack_message(\"Failure: {0}: {1}\".format(kwargs['ts'], kwargs['task']))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def keyword_example(**kwargs):\n",
    "    slack_message(\"{0} - {1} \".format(kwargs['ds'], kwargs['task_instance']))\n",
    "\n",
    "\n",
    "def combined_keywords(ds, key, **kwargs):  # Explicitly reference the used keywords\n",
    "    slack_message(\"{0} - {1} \".format(ds, key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Recommended to import main python scripts from an external file, e.g.: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sys.path.append('/home/airflow/etl/')\n",
    "import etl_script\n",
    "\n",
    "def func_called_from_tast(ds, **kwargs):\n",
    "    etl_script.some_function(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flow Controls\n",
    "#### Set tasks up and downstream of eachother"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# The following two commands are functionally identical\n",
    "task_a.set_upstream(task_b)\n",
    "task_b.set_upstream(task_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Branch Operators allow a job to switch based on an external condition\n",
    "##### e.g. We ran an operation that would run differently when the system date was on a Saturday"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def decide_which_path():\n",
    "    if something is True:\n",
    "        return \"branch_a\"\n",
    "    else:\n",
    "        return \"branch_b\"\n",
    "\n",
    "branch_task = BranchPythonOperator(\n",
    "    task_id='run_this_first',\n",
    "    python_callable=decide_which_path,\n",
    "    trigger_rule=\"all_done\",\n",
    "    dag=dag)\n",
    "\n",
    "branch_task.set_downstream(branch_a)\n",
    "branch_task.set_downstream(branch_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dummy operators act as placeholders when designing workflows.\n",
    "##### These are particularly useful when you want to branch to nowhere or \n",
    "##### have a number of jobs all intentionally bottleneck for dependencies."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "branch_b = DummyOperator(\n",
    "    task_id='branch_b',\n",
    "    dag=dag,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trigger Rules\n",
    "Defaults to requireing all upstream jobs to success: `all_success`\n",
    "\n",
    "Possible other options are: `{ all_success | all_failed | all_done | one_success | one_failed | dummy }`\n",
    "https://airflow.apache.org/concepts.html#trigger-rules\n",
    "\n",
    "Unless a complicated stream is required involving failures, I recommend using `on_failure` to catch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practices / Lessons Learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Default Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Executors\n",
    "There are multiple executors available in the airflow configuration. The default is the Sequential Executor, which can only operate on one task at a time across all DAGs. This is the only option when logging to SQLite (see below)\n",
    "\n",
    "Local Executor will allow multiple tasks to run on Airflow's host. There are additional executors available for distributed systems using Celery or Dask.\n",
    "\n",
    "https://airflow.incubator.apache.org/code.html?#executors\n",
    "\n",
    "https://airflow.apache.org/configuration.html#scaling-out-with-celery\n",
    "\n",
    "https://airflow.apache.org/configuration.html#scaling-out-with-dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logging\n",
    "The webserver maintains logs in a database configured by default to use a local SQLite database. It is recommended to connect to an external SQL database that allows for concurrency, which will open up the Local Executor or one of the distributed Executors\n",
    "\n",
    "The text output of the scripts defaults to a local logs directory. This can also be configured to dump logs directly to an S3 bucket.\n",
    "\n",
    "https://airflow.apache.org/configuration.html#logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  #### Our Own Experiences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passed timestamps/datestamps are all \"one run previous\";\n",
    "i.e. If set to daily, a dag set to run on 2018-03-15 will have a `ds` of 2018-03-14\n",
    "     likewise, if the job is set to run hourly, when it runs at 9am, the timestamp will indicate 8am "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The on_failure task keywords do not contain any error information. If you're looking to pass some details of the error to an outbound message (e.g. Slack, e-mail), you should catch the errors and pass through an embedded message and then re-raise an appropriate exception."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For persistence, the webserver and scheduler should be run with the `-D` command; however it is not consistent, and it is recommended to run as a screen when running on a Linux server, or some other method to maintain persistence while disconnected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Airflow has no built in restart functionality.\n",
    "We run a cronjob to monitor running tasks that match the `scheduler` and `webserver` process names, and trigger a bash script to restart the affected process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Airflow has operators for running SQL directly, and for connecting to an e-mail servers or S3 through `hooks`. We migrate self-contained Python scripts with embedded SQL and S3 connections. We run all our tasks through PythonOperators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros/Cons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pros\n",
    "- Easy to setup\n",
    "- Web Interface for management/tracking\n",
    "- Can run basically anything arbitrarily using Python scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cons\n",
    "- Still in beta\n",
    "- Prone to bugs\n",
    "- Does not self-manage startup/restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
